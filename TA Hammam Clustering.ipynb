{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Git Repo Hammam Tugas Akhir\r\n",
    "import csv\r\n",
    "import regex\r\n",
    "import datetime as dt\r\n",
    "from datetime import timedelta\r\n",
    "import locale\r\n",
    "locale.setlocale(locale.LC_TIME, 'id-ID.UTF-8')\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from scipy import stats\r\n",
    "from scipy.spatial.distance import cdist\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "sns.set_style(\"whitegrid\")\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")\r\n",
    "#from feature_engine.outlier_removers import Winsorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Coret-Coret Marketing_Transaction Details (TA Hammam)_2006-2105.csv\")\n",
    "# df = pd.read_csv(\"Coret-Coret Marketing_Transaction Details (TA Hammam)_2003-2105.csv\")\n",
    "# df = pd.read_csv(\"Coret-Coret Marketing_Transaction Details (TA Hammam)_1904-2104.csv\")\n",
    "display(df.info())\n",
    "df.to_csv(\"Hasil/Data_TA.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset dimension : \" + str(df.shape))\n",
    "print('Jumlah Customer : ' + str(len(pd.unique(df['telp']))))\n",
    "print('Jumlah Transaksi : ' + str(len(pd.unique(df['no_order']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean unused atribute\n",
    "df.drop(['transaction_type', 'ID CL', 'nama_komunitas', 'username', 'useremail', 'prod_id', 'harga', 'qty'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Telp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Null\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#standarize telp number\n",
    "df['telp'] = df['telp'].str.replace('-','')\n",
    "df['telp'] = df['telp'].str.replace(' ','')\n",
    "df['telp'] = df['telp'].str.replace('+','')\n",
    "df['telp'] = df['telp'].str.replace('08', '628', 1)\n",
    "df = df[df['telp'].str.startswith('628')]\n",
    "\n",
    "#Clean admin no hp & Invalid\n",
    "df = df.drop(df[df.telp.eq('6281381306699')\n",
    "                | df.telp.eq('0')].index)\n",
    "df = df[~df['telp'].str.contains('1234567')]\n",
    "\n",
    "#drop number with invalid character\n",
    "df = df.drop(df[df.telp.str.contains(r'[^0-9a-zA-Z]')\n",
    "                | df.telp.str.contains(r'[a-zA-Z]')].index)\n",
    "                \n",
    "#drop abnormal telpon length\n",
    "df = df[df['telp'].str.len() > 10]\n",
    "df = df[df['telp'].str.len() < 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Name, Channel, Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean penyesuaian, test, dan keperluan internal\n",
    "testing = ['test', 'testing', 'COBAAA', 'tes123', 'TEXTING', 'penyesuaian', 'penyelamatan', 'stok', 'stock', 'tamu', 'b2b']\n",
    "df = df[~df['name'].str.contains('|'.join(testing))]\n",
    "\n",
    "#Clean Old B2B Customer\n",
    "B2B_cust = ['hotel', 'resto', 'restoran', 'cafe', 'coffee', 'Toko', 'toko', 'Patata', 'Geprek', 'Laziza', 'ayam', 'nasi', 'sego', 'Baksoe', 'Bakso', 'Rismart', 'Nu Mart', 'Warung', 'Rice box','E-Warung', 'ewarung', 'Kedai', 'geprek', 'nelongso', 'pkk', 'pkh']\n",
    "df = df[~df['name'].str.contains('|'.join(B2B_cust))] \n",
    "\n",
    "#Clean B2B Product\n",
    "B2Bproduct = ['B2B']\n",
    "df = df[~df['produk'].str.contains('|'.join(B2Bproduct))] \n",
    "\n",
    "#Clean Based on Channel\n",
    "channel = ['CL', 'End User', 'Shopee']\n",
    "df = df[df['Channel'].isin(channel)]\n",
    "\n",
    "#clean status non delivered\n",
    "df = df[df.delivery_status.eq('delivered')]\n",
    "\n",
    "#Cleaning outlier (Paket donasi sembako dan grosir)\n",
    "## need futher data based technique\n",
    "donasi = ['Donasi','donasi','Pahlawan', 'Pangan', 'garda', 'sembako', 'psbb ']\n",
    "df = df[~df['name'].str.contains('|'.join(donasi))]\n",
    "df = df[(df['subtotal'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset dimension : \" + str(df.shape))\n",
    "print('Jumlah Customer : ' + str(len(pd.unique(df['telp']))))\n",
    "print('Jumlah Transaksi : ' + str(len(pd.unique(df['no_order']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse datetime format\n",
    "df_clean = df.copy()\n",
    "df_clean['delivery_date'] = df_clean['delivery_date'].apply(lambda x:dt.datetime.strptime(x,'%d %b %Y'))\n",
    "\n",
    "df_clean.to_csv(\"Hasil/Data_TA Clean.csv\")\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate LRFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by invoice \n",
    "df_group = df_clean.groupby(['no_order', 'telp', 'delivery_date'], as_index = False).agg({'subtotal':'sum'})\n",
    "df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By Telp\n",
    "#Perform mapping to LRFM\n",
    "snapshot_date = df_clean['delivery_date'].max() + timedelta(days=1)\n",
    "lrfm = df_group.groupby(['telp']).agg(Length=('delivery_date', lambda x: (snapshot_date - x.min()).days),\n",
    "                                   Recency=('delivery_date', lambda x: (snapshot_date - x.max()).days),\n",
    "                                   Frequency=('no_order', 'count'),\n",
    "                                   Monetary=('subtotal', 'sum'))\n",
    "lrfm.to_csv(\"Hasil/Data_TA LRFM.csv\")\n",
    "display(lrfm)\n",
    "lrfm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Outlier on Frequency\n",
    "# display(sns.boxplot(lrfm['Frequency']))\n",
    "\n",
    "# #delete outlier\n",
    "# lrfm = lrfm[~(lrfm['Frequency'] > 57)]\n",
    "# sns.boxplot(x=lrfm['Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_skew(df_skew, column):\n",
    "    skew = stats.skew(df_skew[column])\n",
    "    skewtest = stats.skewtest(df_skew[column])\n",
    "    plt.title('Distribution of ' + column)\n",
    "    sns.distplot(df_skew[column], kde=False)\n",
    "    \n",
    "    print(\"{}'s: Skew: {}, : {}\".format(column, skew, skewtest))\n",
    "    return\n",
    "\n",
    "def norm_minmax(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all 4 graphs to check skewness\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "check_skew(lrfm,'Length')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "check_skew(lrfm,'Recency')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "check_skew(lrfm,'Frequency')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "check_skew(lrfm,'Monetary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Grafik/before_transform.png', format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency dan monetary skewnya ndak masuk akal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfm_trans = lrfm.copy()\n",
    "\n",
    "lrfm_trans['Recency'] = np.sqrt(lrfm_trans['Recency'])\n",
    "lrfm_trans['Frequency'] = np.log10(lrfm_trans['Frequency']+1)\n",
    "lrfm_trans['Monetary'] = np.log10(lrfm_trans['Monetary']+1)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(4, 1, 1)\n",
    "check_skew(lrfm_trans,'Length')\n",
    "plt.subplot(4, 1, 2)\n",
    "check_skew(lrfm_trans,'Recency')\n",
    "plt.subplot(4, 1, 3)\n",
    "check_skew(lrfm_trans,'Frequency')\n",
    "plt.subplot(4, 1, 4)\n",
    "check_skew(lrfm_trans,'Monetary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Grafik/after_transform-1.png', format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfm_trans['Frequency'] = np.sqrt(lrfm_trans['Frequency'])\n",
    "lrfm_trans['Monetary'] = np.sqrt(lrfm_trans['Monetary'])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(4, 1, 1)\n",
    "check_skew(lrfm_trans,'Length')\n",
    "plt.subplot(4, 1, 2)\n",
    "check_skew(lrfm_trans,'Recency')\n",
    "plt.subplot(4, 1, 3)\n",
    "check_skew(lrfm_trans,'Frequency')\n",
    "plt.subplot(4, 1, 4)\n",
    "check_skew(lrfm_trans,'Monetary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Grafik/after_transform-2.png', format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfm_trans['Frequency'] = np.sqrt(lrfm_trans['Frequency'])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(4, 1, 1)\n",
    "check_skew(lrfm_trans,'Length')\n",
    "plt.subplot(4, 1, 2)\n",
    "check_skew(lrfm_trans,'Recency')\n",
    "plt.subplot(4, 1, 3)\n",
    "check_skew(lrfm_trans,'Frequency')\n",
    "plt.subplot(4, 1, 4)\n",
    "check_skew(lrfm_trans,'Monetary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Grafik/after_transform-3.png', format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfm_trans['Frequency'] = np.sqrt(lrfm_trans['Frequency'])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(4, 1, 1)\n",
    "check_skew(lrfm_trans,'Length')\n",
    "plt.subplot(4, 1, 2)\n",
    "check_skew(lrfm_trans,'Recency')\n",
    "plt.subplot(4, 1, 3)\n",
    "check_skew(lrfm_trans,'Frequency')\n",
    "plt.subplot(4, 1, 4)\n",
    "check_skew(lrfm_trans,'Monetary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Grafik/after_transform-4.png', format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min-Max Normalization\n",
    "norm_lrfm = norm_minmax(lrfm_trans)\n",
    "\n",
    "#Mengubah nilai Recency menggunakan 1-R karena merupakan kebalikan dari variabel lain\n",
    "#R asli jika semakin kecil akan semakin bagus\n",
    "norm_lrfm['Recency'] = 1-norm_lrfm['Recency']\n",
    "\n",
    "norm_lrfm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "from kneed import KneeLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find K Optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow method Viz\r\n",
    "K_elbow = range(1,10)\r\n",
    "inertias = []\r\n",
    "distortions = []\r\n",
    "\r\n",
    "for k in K_elbow: \r\n",
    "    #Building and fitting the model \r\n",
    "    kmeanModel = KMeans(n_clusters=k, n_init=10, max_iter=100, random_state=123)\r\n",
    "    kmeanModel.fit(norm_lrfm)     \r\n",
    "\r\n",
    "    #Elbow Method metrics\r\n",
    "    inertias.append(kmeanModel.inertia_) \r\n",
    "    distortions.append(sum(np.min(cdist(lrfm, kmeanModel.cluster_centers_, \r\n",
    "                      'euclidean'),axis=1)) / norm_lrfm.shape[0])     \r\n",
    "\r\n",
    "kn = KneeLocator(K_elbow, inertias, curve='convex', direction='decreasing')\r\n",
    "print(\"Elbow at K =\", kn.knee)\r\n",
    "\r\n",
    "plt.plot(K_elbow, inertias, 'bx-') \r\n",
    "plt.xlabel('Values of K') \r\n",
    "plt.ylabel('SSE') \r\n",
    "plt.title('The Elbow Method using SSE') \r\n",
    "plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\r\n",
    "plt.savefig('Grafik/Elbow Method using Inertia.png', format='png', dpi=1000)\r\n",
    "plt.show() \r\n",
    "\r\n",
    "kn = KneeLocator(K_elbow, distortions, curve='convex', direction='decreasing')\r\n",
    "print(\"Elbow at K =\", kn.knee)\r\n",
    "\r\n",
    "plt.plot(K_elbow, distortions, 'bx-') \r\n",
    "plt.xlabel('Values of K') \r\n",
    "plt.ylabel('Distortion') \r\n",
    "plt.title('The Elbow Method using SSE') \r\n",
    "plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\r\n",
    "plt.savefig('Grafik/Elbow Method using distortion.png', format='png', dpi=1000)\r\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(2,10)\r\n",
    "# fig, ax = plt.subplots(4, 2, figsize=(15,15))\r\n",
    "\r\n",
    "# Silhouette Method metrics\r\n",
    "euclidean = []\r\n",
    "ch_index = []\r\n",
    "db_index = []\r\n",
    "  \r\n",
    "for k in K: \r\n",
    "    #Building and fitting the model \r\n",
    "    kmeanModel = KMeans(n_clusters=k, n_init=10, max_iter=100, random_state=123)\r\n",
    "    kmeanModel.fit(norm_lrfm)     \r\n",
    "    \r\n",
    "    # Silhouette Method metrics\r\n",
    "    euclidean.append(silhouette_score(norm_lrfm, kmeanModel.labels_, metric='euclidean'))\r\n",
    "\r\n",
    "    # Silhouette also\r\n",
    "    # q, mod = divmod(k, 2)\r\n",
    "    # visualizer = SilhouetteVisualizer(kmeanModel, colors='yellowbrick', ax=ax[q-1][mod])\r\n",
    "    # visualizer.fit(norm_lrfm)\r\n",
    "\r\n",
    "    # C-H Index\r\n",
    "    ch_index.append(calinski_harabasz_score(norm_lrfm,kmeanModel.labels_))\r\n",
    "\r\n",
    "    # DB Index\r\n",
    "    db_index.append(davies_bouldin_score(norm_lrfm,kmeanModel.labels_))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymax = max(euclidean)\n",
    "xpos = euclidean.index(ymax)\n",
    "xmax = K[xpos]\n",
    "\n",
    "print(\"Highest Euclidean Value = %s at K=%s\" % (ymax, xmax,))\n",
    "plt.plot(K, euclidean, 'bo-') \n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Euclidean') \n",
    "plt.title('The Silhouette Method using Euclidean Distance') \n",
    "plt.vlines(xmax, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
    "plt.savefig('Grafik/Silhouette Method using Euclidean Distance.png', format='png', dpi=1000)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CH Index Viz\n",
    "ymax = max(ch_index)\n",
    "xpos = ch_index.index(ymax)\n",
    "xmax = K[xpos]\n",
    "\n",
    "print(\"Highest CH-Index Value = %s at K=%s\" % (ymax, xmax,))\n",
    "plt.plot(K, ch_index, 'bo-') \n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('CH Index') \n",
    "plt.title('CH Index for k=2 to k=9') \n",
    "plt.vlines(xmax, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
    "plt.savefig('Grafik/CH Index.png', format='png', dpi=1000)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Index Viz\n",
    "ymin = min(db_index)\n",
    "xpos = db_index.index(ymin)\n",
    "xmax = K[xpos]\n",
    "\n",
    "print(\"Lowest DB Index Value = %s at K=%s\" % (ymax, xmax,))\n",
    "plt.plot(K, db_index, 'bo-') \n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('DB Index') \n",
    "plt.title('Davis Bouldin Index for k=2 to k=9') \n",
    "plt.vlines(xmax, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set K Value\r\n",
    "K_fix = 3\r\n",
    "\r\n",
    "#Scatter Plot\r\n",
    "# plt.figure(figsize=(16, 8))\r\n",
    "df_cluster_fix = kmeans(norm_lrfm, K_fix, lrfm)\r\n",
    "# plt.savefig('Grafik/Cluster_is_4-flattened.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lrfm_all = pd.merge(df_cluster_fix, norm_lrfm, on='telp', suffixes=('_real', '_norm'))\r\n",
    "\r\n",
    "df_lrfm_all.to_csv(\"Hasil/Data_TA Clustered_LRFM.csv\")\r\n",
    "df_lrfm_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(normalised_df_lrfm, clusters_number, original_df_lrfm):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = clusters_number, random_state = 123)\n",
    "    kmeans.fit(normalised_df_lrfm)\n",
    "\n",
    "    # Extract cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "        \n",
    "    # Create a cluster label column in original dataset\n",
    "    df_new = original_df_lrfm.assign(Cluster = cluster_labels)\n",
    "    df_new['Cluster'] += 1\n",
    "\n",
    "    # # Initialise TSNE\n",
    "    # model = TSNE(random_state=1)\n",
    "    # transformed = model.fit_transform(df_new)\n",
    "    \n",
    "    # # Plot t-SNE\n",
    "    # plt.title('Flattened Graph of {} Clusters'.format(clusters_number))\n",
    "    # sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=cluster_labels, style=cluster_labels, palette=\"Set1\")\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def snake_plot(normalised_df_lrfm, df_lrfm_kmeans, df_lrfm_original):\n",
    "\n",
    "    normalised_df_lrfm = pd.DataFrame(normalised_df_lrfm, \n",
    "                                       index=df_lrfm_original.index, \n",
    "                                       columns=df_lrfm_original.columns)\n",
    "    normalised_df_lrfm['Cluster'] = df_lrfm_kmeans['Cluster']\n",
    "\n",
    "    # Melt data into long format\n",
    "    df_melt = pd.melt(normalised_df_lrfm.reset_index(), \n",
    "                        id_vars=['telp', 'Cluster'],\n",
    "                        value_vars=['Length', 'Recency', 'Frequency', 'Monetary'], \n",
    "                        var_name='Metric', \n",
    "                        value_name='Value')\n",
    "\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Value')\n",
    "    sns.pointplot(data=df_melt, x='Metric', y='Value', hue='Cluster')\n",
    "    \n",
    "    return\n",
    "\n",
    "def threeD_plot(df_merge):\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    x = np.array(df_merge['Recency_norm'])\n",
    "    y = np.array(df_merge['Frequency_norm'])\n",
    "    z = np.array(df_merge['Monetary_norm'])\n",
    "\n",
    "    plt.xlabel('Recency')\n",
    "    plt.ylabel('Frequency')\n",
    "    # plt.zlabel('Monetary')\n",
    "    \n",
    "    ax.scatter(x,y,z, s=(df_merge['Length_norm']*40), marker=\"s\", c=df_merge[\"Cluster\"], cmap=\"rainbow\")\n",
    "    # scatter = ax.scatter(x, y, c=c, s=s) \n",
    "    # legend = ax.legend(*scatter.legend_elements(),\n",
    "    #                 loc=\"lower left\", title=\"Classes\")\n",
    "    # ax.add_artist(legend)\n",
    "    # ax.legend(loc='upper left', numpoints=1, ncol=3, fontsize=8, bbox_to_anchor=(0, 0)) \n",
    "\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "#Snake Plot\n",
    "plt.title(\"Snake Plot of K = %s\" % (K_fix))\n",
    "snake_plot(norm_lrfm, df_cluster_fix, lrfm)\n",
    "plt.savefig('Grafik/Cluster_is_3-snakeplot.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "#3D Plot\n",
    "ax = plt.axes(projection='3d')\n",
    "plt.title(\"3D Plot of K = %s\" % (K_fix))\n",
    "threeD_plot(df_lrfm_all)\n",
    "plt.savefig('Grafik/Cluster_is_3-3dplot.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoxPlot for Real Data\r\n",
    "fig = plt.figure(figsize=(10, 5))\r\n",
    "ax = sns.boxplot(x=\"Cluster\", y=\"Length_real\", data=df_lrfm_all, width=0.3, whis=10)\r\n",
    "L_mean = df_lrfm_all[\"Length_real\"].mean()\r\n",
    "plt.plot([-0.5, 2.5], [L_mean, L_mean], 'k-', lw=2, dashes=[5, 2], color='purple')\r\n",
    "plt.title('Boxplot for Length each cluster') \r\n",
    "plt.savefig('Grafik/boxplot L.png', format='png', dpi=300)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(10, 5))\r\n",
    "ax = sns.boxplot(x=\"Cluster\", y=\"Recency_real\", data=df_lrfm_all, width=0.3, whis=10)\r\n",
    "R_mean = df_lrfm_all[\"Recency_real\"].mean()\r\n",
    "plt.plot([0, 2], [R_mean, R_mean], 'k-', lw=2, dashes=[5, 2], color='purple')\r\n",
    "plt.title('Boxplot for Recency each cluster') \r\n",
    "plt.savefig('Grafik/boxplot R.png', format='png', dpi=300)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(10, 5))\r\n",
    "plt.yscale(\"log\")\r\n",
    "ax = sns.boxplot(x=\"Cluster\", y=\"Frequency_real\", data=df_lrfm_all, width=0.3, whis=100)\r\n",
    "F_mean = df_lrfm_all[\"Frequency_real\"].mean()\r\n",
    "plt.plot([0, 2], [F_mean, F_mean], 'k-', lw=2, dashes=[5, 2], color='purple')\r\n",
    "plt.title('Boxplot for Frequency each cluster') \r\n",
    "plt.savefig('Grafik/boxplot F.png', format='png', dpi=300)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(10, 5))\r\n",
    "plt.yscale(\"log\")\r\n",
    "ax = sns.boxplot(x=\"Cluster\", y=\"Monetary_real\", data=df_lrfm_all, width=0.3, whis=100)\r\n",
    "M_mean = df_lrfm_all[\"Monetary_real\"].mean()\r\n",
    "plt.plot([0, 2], [M_mean, M_mean], 'k-', lw=2, dashes=[5, 2], color='purple')\r\n",
    "plt.title('Boxplot for Monetary each cluster') \r\n",
    "plt.savefig('Grafik/boxplot M.png', format='png', dpi=300)\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot for Normalized data\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = sns.boxplot(x=\"Cluster\", y=\"Length_norm\", data=df_lrfm_all)\n",
    "plt.title('Boxplot for Length each cluster') \n",
    "plt.savefig('Grafik/Cluster_is_3-L_boxplot_norm.png', format='png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = sns.boxplot(x=\"Cluster\", y=\"Recency_norm\", data=df_lrfm_all)\n",
    "plt.title('Boxplot for Recency each cluster') \n",
    "plt.savefig('Grafik/Cluster_is_3-R_boxplot_norm.png', format='png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = sns.boxplot(x=\"Cluster\", y=\"Frequency_norm\", data=df_lrfm_all)\n",
    "plt.title('Boxplot for Frequency each cluster') \n",
    "plt.savefig('Grafik/Cluster_is_3-F_boxplot_norm.png', format='png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = sns.boxplot(x=\"Cluster\", y=\"Monetary_norm\", data=df_lrfm_all)\n",
    "plt.title('Boxplot for Monetary each cluster') \n",
    "plt.savefig('Grafik/Cluster_is_3-M_boxplot_norm.png', format='png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violin Plot\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x='Cluster', y='Length_real', data=df_lrfm_all, scale='width', inner='quartile')\n",
    "plt.title('Violin Plot of Length each cluster', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x='Cluster', y='Recency_real', data=df_lrfm_all, scale='width', inner='quartile')\n",
    "plt.title('Violin Plot of Recency each cluster', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x='Cluster', y='Frequency_real', data=df_lrfm_all, scale='width', inner='quartile')\n",
    "plt.yscale(\"log\")\n",
    "plt.title('Violin Plot of Frequency each cluster (log scale)', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x='Cluster', y='Monetary_real', data=df_lrfm_all, scale='width', inner='quartile')\n",
    "plt.yscale(\"log\")\n",
    "plt.title('Violin Plot of Monetary each cluster (log scale)', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('Grafik/Cluster_is_3-Violin.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Violin Plot\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x='Cluster', y='Length_norm', data=df_lrfm_all, scale='width', inner='quartile')\n",
    "plt.title('Violin Plot of Normalized Length each cluster', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x='Cluster', y='Recency_norm', data=df_lrfm_all, scale='width', inner='quartile')\n",
    "plt.title('Violin Plot of Normalized Recency each cluster', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x='Cluster', y='Frequency_norm', data=df_lrfm_all, scale='width', inner='quartile')\n",
    "plt.title('Violin Plot of Normalized Frequency each cluster', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.violinplot(x='Cluster', y='Monetary_norm', data=df_lrfm_all, scale='width', inner='quartile')\n",
    "plt.title('Violin Plot of Normalized Monetary each cluster', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('Grafik/Cluster_is_3-Violin_norm.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_char = df_cluster_fix.reset_index().groupby(['Cluster']).describe()\n",
    "cluster_char.transpose().to_csv(\"Hasil/Data_TA Cluster_characteristics.csv\")\n",
    "cluster_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_fix.groupby(['Cluster']).agg('mean').round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean from all dataframe\n",
    "df_lrfm_all.mean(axis=0).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Menghitung CLV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nilai bobot didapatkan dari file excel\n",
    "AHP = {\n",
    "    'Length' : 0.052,\n",
    "    'Recency' : 0.099,\n",
    "    'Frequency' : 0.362,\n",
    "    'Monetary' : 0.486}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lrfm_all['CLV'] = (df_lrfm_all['Length_norm']*AHP['Length'] + df_lrfm_all['Recency_norm']*AHP['Recency'] + df_lrfm_all['Frequency_norm']*AHP['Frequency'] + df_lrfm_all['Monetary_norm']*AHP['Monetary'])\n",
    "display(df_lrfm_all['CLV'].describe())\n",
    "\n",
    "#Agar angka lebih cantik dan mudah dibaca, kita kalikan 1000\n",
    "df_lrfm_all['CLV'] = df_lrfm_all['CLV']*100\n",
    "plt.title('Distribution of CLV')\n",
    "sns.distplot(df_lrfm_all['CLV'], kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lrfm_all.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank CLV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lrfm_all.groupby(['Cluster']).agg('mean').sort_values(by=['CLV'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clv_values(df):\n",
    "    clv_cluster = df.groupby(['Cluster']).agg({\n",
    "        'Length_real': ['min', 'max', 'mean'],\n",
    "        'Recency_real': ['min', 'max', 'mean'],\n",
    "        'Frequency_real': ['min', 'max', 'mean'],\n",
    "        'Monetary_real': ['min', 'max', 'mean'],\n",
    "        'CLV' : ['min', 'max', 'mean', 'median']\n",
    "    }).round(0)\n",
    "    \n",
    "    return clv_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_values(df_lrfm_all).sort_values(by=[('CLV','mean')], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Basket Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket_all = pd.merge(df_clean, df_cluster_fix, on='telp', how='left')\r\n",
    "df_basket_all = df_basket_all[['delivery_date', 'no_order', 'name', 'telp', 'produk', 'qty', 'Cluster']].copy()\r\n",
    "df_basket_all.to_csv(\"Hasil/Data_TA Clustered_Transaksi.csv\")\r\n",
    "\r\n",
    "#drop NA from frequency outlier\r\n",
    "df_basket_all.dropna(subset=['Cluster'], inplace=True)\r\n",
    "\r\n",
    "print(\"Dataset dimension : \" + str(df_basket_all.shape))\r\n",
    "print('Jumlah Customer : ' + str(len(pd.unique(df_basket_all['telp']))))\r\n",
    "print('Jumlah Transaksi : ' + str(len(pd.unique(df_basket_all['no_order']))))\r\n",
    "\r\n",
    "#clean the product names\r\n",
    "df_basket_all['produk'] = df_basket_all['produk'].str.strip()\r\n",
    "df_basket_all['no_order'] = df_basket_all['no_order'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode the basket\r\n",
    "def encode_units(x):\r\n",
    "    if x <= 0:\r\n",
    "        return 0\r\n",
    "    if x >= 1:\r\n",
    "        return 1\r\n",
    "\r\n",
    "#create FP-Growth MBA for every cluster\r\n",
    "def createMBA(basket_data, min_sup) :\r\n",
    "    totalTransactions = len(basket_data.index)\r\n",
    "    totalCustomers = len(pd.unique(basket_data['telp']))\r\n",
    "    # minTransaction = totalTransactions*0.005\r\n",
    "    # min_support_calc = minTransaction/totalTransactions\r\n",
    "\r\n",
    "    print('Jumlah Pelanggan = ', totalCustomers)\r\n",
    "    print('Jumlah Transaksi yang dianalisis = ', totalTransactions)\r\n",
    "    print('Nilai Support Minimum = ', round(min_sup*100, 4), '%')\r\n",
    "\r\n",
    "    basket = basket_data.groupby(['no_order', 'produk'])['qty'].sum().unstack().reset_index().fillna(0).set_index('no_order')\r\n",
    "    basket_sets = basket.applymap(encode_units)\r\n",
    "    basket_sets.dropna(inplace=True)\r\n",
    "    basket_sets = basket_sets.astype(int)\r\n",
    "    # display(basket_sets.head(5))\r\n",
    "\r\n",
    "    #create frequent items sets with clculated minimum support\r\n",
    "    frequent_itemsets = fpgrowth(basket_sets, min_support=min_sup, use_colnames=True)\r\n",
    "    # display(frequent_itemsets.describe())\r\n",
    "\r\n",
    "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=0)\r\n",
    "    rules.sort_values('support', ascending = False, inplace = True)\r\n",
    "\r\n",
    "    createMBA.rules = rules\r\n",
    "    \r\n",
    "    return rules\r\n",
    "\r\n",
    "#Apriori\r\n",
    "from mlxtend.frequent_patterns import apriori\r\n",
    "\r\n",
    "def aprioriMBA(basket_data, min_sup) :\r\n",
    "    totalTransactions = len(basket_data.index)\r\n",
    "    totalCustomers = len(pd.unique(basket_data['telp']))\r\n",
    "    # minTransaction = totalTransactions*0.005\r\n",
    "    # min_support_calc = minTransaction/totalTransactions\r\n",
    "\r\n",
    "    print('Jumlah Pelanggan = ', totalCustomers)\r\n",
    "    print('Jumlah Transaksi yang dianalisis = ', totalTransactions)\r\n",
    "    print('Nilai Support Minimum = ', round(min_sup*100, 4), '%')\r\n",
    "\r\n",
    "    basket = basket_data.groupby(['no_order', 'produk'])['qty'].sum().unstack().reset_index().fillna(0).set_index('no_order')\r\n",
    "    basket_sets = basket.applymap(encode_units)\r\n",
    "    basket_sets.dropna(inplace=True)\r\n",
    "    basket_sets = basket_sets.astype(int)\r\n",
    "    # display(basket_sets.head(5))\r\n",
    "\r\n",
    "    #create frequent items sets with clculated minimum support\r\n",
    "    frequent_itemsets = apriori(basket_sets, min_support=min_sup, use_colnames=True)\r\n",
    "    # display(frequent_itemsets.describe())\r\n",
    "\r\n",
    "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=0)\r\n",
    "    rules.sort_values('support', ascending = False, inplace = True)\r\n",
    "\r\n",
    "    aprioriMBA.rules = rules\r\n",
    "    \r\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MBA w/ Clustering minsup 1%\r\n",
    "for i in range(1, K_fix+1) :\r\n",
    "    #slice data\r\n",
    "    print(\"Market Basket Analysis for Cluster\", i)\r\n",
    "    basket_data = df_basket_all[lambda x: x['Cluster'] == i]\r\n",
    "\r\n",
    "    createMBA(basket_data, 0.01)\r\n",
    "    mean_suppport = createMBA.rules['support'].mean()\r\n",
    "    print('rules berhasil dibangkitkan = ', len(createMBA.rules.index))\r\n",
    "    print('rerata support = ', mean_suppport)\r\n",
    "    \r\n",
    "    \r\n",
    "    display(createMBA.rules.head(5))\r\n",
    "    createMBA.rules.to_csv(\"Hasil/Data_TA FPGrowth(minsup0.01)_Cluster_%s.csv\" % (i,), ';')\r\n",
    "    # rules[(rules['lift'] >= 0.2) & (rules['confidence'] >= 0.1)].sort_values(by=['confidence', 'lift'], ascending=False).to_csv(\"Data/Ternakmart_Results_FPGrowth_Cluster_%s.csv\" % (i,), 'a', newline='')\r\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apriori MBA w/ Clustering minsup 1%\r\n",
    "for i in range(1, K_fix+1) :\r\n",
    "    #slice data\r\n",
    "    print(\"Market Basket Analysis for Cluster\", i)\r\n",
    "    basket_data = df_basket_all[lambda x: x['Cluster'] == i]\r\n",
    "\r\n",
    "    aprioriMBA(basket_data, 0.01)\r\n",
    "    mean_suppport = aprioriMBA.rules['support'].mean()\r\n",
    "    print('rules berhasil dibangkitkan = ', len(aprioriMBA.rules.index))\r\n",
    "    print('rerata support = ', mean_suppport)\r\n",
    "    \r\n",
    "    display(aprioriMBA.rules.head(5))\r\n",
    "    aprioriMBA.rules.to_csv(\"Hasil/Data_TA Apriori(minsup0.01)_Cluster_%s.csv\" % (i,), ';')\r\n",
    "    # rules[(rules['lift'] >= 0.2) & (rules['confidence'] >= 0.1)].sort_values(by=['confidence', 'lift'], ascending=False).to_csv(\"Data/Ternakmart_Results_FPGrowth_Cluster_%s.csv\" % (i,), 'a', newline='')\r\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MBA w/ Clustering minsup 0.5%\r\n",
    "for i in range(1, K_fix+1) :\r\n",
    "    #slice data\r\n",
    "    print(\"Market Basket Analysis for Cluster\", i)\r\n",
    "    basket_data = df_basket_all[lambda x: x['Cluster'] == i]\r\n",
    "\r\n",
    "    createMBA(basket_data, 0.005)\r\n",
    "    print('rules berhasil dibangkitkan = ', len(createMBA.rules.index))\r\n",
    "\r\n",
    "    display(createMBA.rules.head(10))\r\n",
    "\r\n",
    "    plt.scatter(createMBA.rules['support'], createMBA.rules['confidence'], alpha=0.5)\r\n",
    "    # plt.xlabel('support')\r\n",
    "    # plt.ylabel('confidence')\r\n",
    "    # plt.title('Support vs Confidence')\r\n",
    "    # plt.show()\r\n",
    "\r\n",
    "    createMBA.rules.to_csv(\"Hasil/Data_TA FPGrowth (minsup0.005)_Cluster_%s.csv\" % (i,), ';')\r\n",
    "    # rules[(rules['lift'] >= 0.2) & (rules['confidence'] >= 0.1)].sort_values(by=['confidence', 'lift'], ascending=False).to_csv(\"Data/Ternakmart_Results_FPGrowth_Cluster_%s.csv\" % (i,), 'a', newline='')\r\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MBA w/o Clustering\n",
    "createMBA(df_basket_all, 0.01)\n",
    "print('rules berhasil dibangkitkan = ', len(createMBA.rules.index))\n",
    "\n",
    "display(createMBA.rules.head(10))\n",
    "createMBA.rules.to_csv(\"Hasil/Data_EXP FPGrowth_NO_Cluster_%s.csv\", ';')\n",
    "# rules[(rules['lift'] >= 0.2) & (rules['confidence'] >= 0.1)].sort_values(by=['confidence', 'lift'], ascending=False).to_csv(\"Data/Ternakmart_Results_FPGrowth_Cluster_All.csv\", 'a', newline='')\n",
    "print(\"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MBA w/o Jabmilk\n",
    "jabmilk = ['JABMILK', 'JAB MILK']\n",
    "df_basket_nonjabmilk = df_basket_all[~df_basket_all['produk'].str.contains('|'.join(jabmilk))]\n",
    "\n",
    "for i in range(1, K_fix+1) :\n",
    "    #slice data\n",
    "    print(\"Market Basket Analysis (Without Jabmilk) for Cluster\", i)\n",
    "    basket_data = df_basket_nonjabmilk[lambda x: x['Cluster'] == i]\n",
    "\n",
    "    createMBA(basket_data, 0.01)\n",
    "    print('rules berhasil dibangkitkan = ', len(createMBA.rules.index))\n",
    "\n",
    "    display(createMBA.rules.head(10))\n",
    "    createMBA.rules.to_csv(\"Hasil/Data_EXP FPGrowth_nonjabmilk_Cluster_%s.csv\" % (i,), ';')\n",
    "    # rules[(rules['lift'] >= 0.2) & (rules['confidence'] >= 0.1)].sort_values(by=['confidence', 'lift'], ascending=False).to_csv(\"Data/Ternakmart_Results_nonjabmilk_FPGrowth_Cluster_%s.csv\" % (i,), 'a', newline='')\n",
    "    print(\"\\n \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MBA w/o Clustering w/o Jabmilk\n",
    "createMBA(df_basket_nonjabmilk, 0.007)\n",
    "print('rules berhasil dibangkitkan = ', len(createMBA.rules.index))\n",
    "createMBA.rules.to_csv(\"Hasil/Data_EXP FPGrowth_nonjabmilk_NO_Cluster_%s.csv\", ';')\n",
    "\n",
    "display(createMBA.rules.head(10))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}