{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import regex\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "import locale\n",
    "locale.setlocale(locale.LC_TIME, 'id-ID.UTF-8')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from kneed import KneeLocator\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#from feature_engine.outlier_removers import Winsorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_skew(df_skew, column):\n",
    "    skew = stats.skew(df_skew[column])\n",
    "    skewtest = stats.skewtest(df_skew[column])\n",
    "    plt.title('Distribution of ' + column)\n",
    "    sns.distplot(df_skew[column], kde=False)\n",
    "    \n",
    "    print(\"{}'s: Skew: {}, : {}\".format(column, skew, skewtest))\n",
    "    return\n",
    "\n",
    "def norm_minmax(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "def snake_plot(normalised_df_lrfm, df_lrfm_kmeans, df_lrfm_original):\n",
    "\n",
    "    normalised_df_lrfm = pd.DataFrame(normalised_df_lrfm, \n",
    "                                       index=df_lrfm_original.index, \n",
    "                                       columns=df_lrfm_original.columns)\n",
    "    normalised_df_lrfm['Cluster'] = df_lrfm_kmeans['Cluster']\n",
    "\n",
    "    # Melt data into long format\n",
    "    df_melt = pd.melt(normalised_df_lrfm.reset_index(), \n",
    "                        id_vars=['telp', 'Cluster'],\n",
    "                        value_vars=['Length', 'Recency', 'Frequency', 'Monetary'], \n",
    "                        var_name='Metric', \n",
    "                        value_name='Value')\n",
    "\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Value')\n",
    "    sns.pointplot(data=df_melt, x='Metric', y='Value', hue='Cluster')\n",
    "    \n",
    "    return\n",
    "\n",
    "def threeD_plot(df_lrfm_kmeans, df_lrfm_original):\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    df_merge = pd.merge(df_lrfm_kmeans, df_lrfm_original['Cluster'], on='telp', how='left')\n",
    "\n",
    "    x = np.array(df_merge['Recency'])\n",
    "    y = np.array(df_merge['Frequency'])\n",
    "    z = np.array(df_merge['Monetary'])\n",
    "\n",
    "    plt.xlabel('Recency')\n",
    "    plt.ylabel('Frequency')\n",
    "    # plt.zlabel('Monetary')  \n",
    "    ax.legend('Cluster')\n",
    "    ax.scatter(x,y,z, s=(df_merge['Length']*40), marker=\"s\", c=df_merge[\"Cluster\"], cmap=\"Paired\")\n",
    "\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Coret-Coret Marketing_Transaction Details (TA Hammam)_2003-2105.csv\")\n",
    "# df = pd.read_csv(\"Coret-Coret Marketing_Transaction Details (TA Hammam)_1904-2104.csv\")\n",
    "display(df.info())\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset dimension : \" + str(df.shape))\n",
    "print('Jumlah Customer : ' + str(len(pd.unique(df['telp']))))\n",
    "print('Jumlah Transaksi : ' + str(len(pd.unique(df['no_order']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Telp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Null\n",
    "df.dropna(subset=['telp', 'produk', 'name'], inplace=True)\n",
    "\n",
    "#standarize telp number\n",
    "df['telp'] = df['telp'].str.replace('-','')\n",
    "df['telp'] = df['telp'].str.replace(' ','')\n",
    "df['telp'] = df['telp'].str.replace('+','')\n",
    "df['telp'] = df['telp'].str.replace('08', '628', 1)\n",
    "df = df[df['telp'].str.startswith('628')]\n",
    "\n",
    "#Clean admin no hp & Invalid\n",
    "df = df.drop(df[df.telp.eq('6281381306699')\n",
    "                | df.telp.eq('0')].index)\n",
    "df = df[~df['telp'].str.contains('1234567')] #<--- Belum Bisa\n",
    "\n",
    "#drop number with invalid character\n",
    "df = df.drop(df[df.telp.str.contains(r'[^0-9a-zA-Z]')\n",
    "                | df.telp.str.contains(r'[a-zA-Z]')].index)\n",
    "                \n",
    "#drop abnormal telpon length\n",
    "df = df[df['telp'].str.len() > 10]\n",
    "df = df[df['telp'].str.len() < 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Name, Channel, Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean penyesuaian, test, dan keperluan internal\n",
    "testing = ['test', 'COBAAA', 'tes123', 'TEXTING', 'penyesuaian', 'penyelamatan', 'stok', 'stock', 'tamu']\n",
    "df = df[~df['name'].str.contains('|'.join(testing))]\n",
    "\n",
    "#Clean Old B2B Customer\n",
    "B2B_cust = ['hotel', 'resto', 'restoran', 'cafe', 'coffee', 'Toko', 'toko', 'Patata', 'Geprek', 'Laziza', 'ayam', 'nasi', 'sego', 'Baksoe', 'Bakso', 'Rismart', 'Nu Mart', 'Warung', 'Rice box','E-Warung', 'ewarung', 'Kedai', 'geprek', 'nelongso', 'pkk', 'pkh']\n",
    "df = df[~df['name'].str.contains('|'.join(B2B_cust))] \n",
    "\n",
    "#Clean B2B Product\n",
    "B2Bproduct = ['B2B']\n",
    "df = df[~df['produk'].str.contains('|'.join(B2Bproduct))] \n",
    "\n",
    "#Clean Produk Promo & Bundling\n",
    "Promo = ['PROMO', 'MAM', 'JSM', 'JUARA', 'PESTA', 'GRATIS', 'PAKET', 'SALE', 'PSBB']\n",
    "df = df[~df['produk'].str.contains('|'.join(Promo))] \n",
    "\n",
    "#Clean Based on Channel\n",
    "channel = ['CL', 'End User', 'Shopee']\n",
    "df = df[df['Channel'].isin(channel)]\n",
    "\n",
    "#clean status non delivered\n",
    "df = df[df.delivery_status.eq('delivered')]\n",
    "\n",
    "#Cleaning outlier (Paket donasi sembako dan grosir)\n",
    "## need futher data based technique\n",
    "donasi = ['donasi', 'garda', 'sembako', 'psbb ']\n",
    "df = df[~df['name'].str.contains('|'.join(testing))]\n",
    "df = df[~(df['subtotal'] > 1500000)]\n",
    "df = df[(df['harga'] > 0)]\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset dimension : \" + str(df.shape))\n",
    "print('Jumlah Customer : ' + str(len(pd.unique(df['telp']))))\n",
    "print('Jumlah Transaksi : ' + str(len(pd.unique(df['no_order']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Hasil/Ternakmart_Transaction_CLEAN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse datetime format\n",
    "df_clean = df.copy()\n",
    "df_clean['delivery_date'] = df_clean['delivery_date'].apply(lambda x:dt.datetime.strptime(x,'%d %b %Y'))\n",
    "df_clean.to_csv(\"Hasil/Ternakmart_Transaction_CLEAN_V1.csv\")\n",
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate LRFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group by invoice \n",
    "df_group = df_clean.groupby(['no_order', 'telp', 'delivery_date'], as_index = False).agg({'subtotal':'sum'})\n",
    "df_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By Telp\n",
    "#Perform mapping to LRFM\n",
    "snapshot_date = df_clean['delivery_date'].max() + timedelta(days=1)\n",
    "lrfm = df_group.groupby(['telp']).agg(Length=('delivery_date', lambda x: (snapshot_date - x.min()).days),\n",
    "                                   Recency=('delivery_date', lambda x: (snapshot_date - x.max()).days),\n",
    "                                   Frequency=('no_order', 'count'),\n",
    "                                   Monetary=('subtotal', 'sum'))\n",
    "lrfm.to_csv(\"Hasil/Ternakmart_Transaction_LRFM.csv\")\n",
    "display(lrfm)\n",
    "lrfm.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Outlier on Frequency\n",
    "sns.boxplot(lrfm['Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfm = lrfm[~(lrfm['Frequency'] > 80)]\n",
    "sns.boxplot(x=lrfm['Frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all 4 graphs to check skewness\n",
    "plt.figure(figsize=(9, 9))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "check_skew(lrfm,'Length')\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "check_skew(lrfm,'Recency')\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "check_skew(lrfm,'Frequency')\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "check_skew(lrfm,'Monetary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Grafik/before_transform.png', format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency dan monetary skewnya ndak masuk akal"
   ]
  },
  {
   "source": [
    "## Transformation & Normalization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfm_trans = lrfm.copy()\n",
    "#log10 Transformation\n",
    "# lrfm_trans['Frequency'] = lrfm_trans['Frequency'].apply(lambda x: 1/x)\n",
    "# lrfm_trans['Monetary'] = lrfm_trans['Monetary'].apply(lambda x: 1/x)\n",
    "\n",
    "lrfm_trans['Recency'] = np.sqrt(lrfm_trans['Recency'])\n",
    "lrfm_trans['Frequency'] = np.log10(lrfm_trans['Frequency']+1)\n",
    "lrfm_trans['Monetary'] = np.log10(lrfm_trans['Monetary']+1)\n",
    "\n",
    "lrfm_trans['Frequency'] = np.sqrt(lrfm_trans['Frequency'])\n",
    "\n",
    "lrfm_trans['Frequency'] = np.sqrt(lrfm_trans['Frequency'])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(4, 1, 1)\n",
    "check_skew(lrfm_trans,'Length')\n",
    "plt.subplot(4, 1, 2)\n",
    "check_skew(lrfm_trans,'Recency')\n",
    "plt.subplot(4, 1, 3)\n",
    "check_skew(lrfm_trans,'Frequency')\n",
    "plt.subplot(4, 1, 4)\n",
    "check_skew(lrfm_trans,'Monetary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Grafik/after_transform.png', format='png', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min-Max Normalization\n",
    "norm_lrfm = norm_minmax(lrfm_trans)\n",
    "norm_lrfm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elbow Method\n",
    "distortions = [] \n",
    "inertias = [] \n",
    "mapping1 = {} \n",
    "mapping2 = {} \n",
    "K = range(2,10) \n",
    "  \n",
    "for k in K: \n",
    "    #Building and fitting the model \n",
    "    kmeanModel = KMeans(n_clusters=k).fit(norm_lrfm) \n",
    "    kmeanModel.fit(norm_lrfm)     \n",
    "      \n",
    "    distortions.append(sum(np.min(cdist(lrfm, kmeanModel.cluster_centers_, \n",
    "                      'euclidean'),axis=1)) / norm_lrfm.shape[0]) \n",
    "    inertias.append(kmeanModel.inertia_) \n",
    "  \n",
    "    mapping1[k] = sum(np.min(cdist(lrfm, kmeanModel.cluster_centers_, \n",
    "                 'euclidean'),axis=1)) / lrfm.shape[0] \n",
    "    mapping2[k] = kmeanModel.inertia_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn = KneeLocator(K, distortions, curve='convex', direction='decreasing')\n",
    "print(\"Elbow at K =\", kn.knee)\n",
    "\n",
    "plt.plot(K, distortions, 'bx-') \n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Distortion') \n",
    "plt.title('The Elbow Method using Distortion') \n",
    "plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
    "plt.savefig('Grafik/Elbow Method using Distortion.png', format='png', dpi=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn = KneeLocator(K, inertias, curve='convex', direction='decreasing')\n",
    "print(\"Elbow at K =\", kn.knee)\n",
    "\n",
    "plt.plot(K, inertias, 'bx-') \n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Inertia') \n",
    "plt.title('The Elbow Method using Inertia/SSE') \n",
    "plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
    "plt.savefig('Grafik/Elbow Method using Inertia.png', format='png', dpi=1000)\n",
    "plt.show() "
   ]
  },
  {
   "source": [
    "## Silhouette Method"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Method\n",
    "euclidean = []\n",
    "cosine = []\n",
    "mapping_euclidean = {} \n",
    "K = range(1,10)\n",
    "\n",
    "# Prepare models\n",
    "for k in K: \n",
    "    kmeans = KMeans(n_clusters=6).fit(norm_lrfm)\n",
    "    normalized_vectors = preprocessing.normalize(norm_lrfm)\n",
    "    normalized_kmeans = KMeans(n_clusters=4).fit(normalized_vectors)\n",
    "    min_samples = norm_lrfm.shape[1]+1\n",
    "\n",
    "    euclidean.append(silhouette_score(norm_lrfm, kmeans.labels_, metric='euclidean'))\n",
    "    cosine.append(silhouette_score(normalized_vectors, normalized_kmeans.labels_, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymax = max(euclidean)\n",
    "xpos = euclidean.index(ymax)\n",
    "xmax = K[xpos]\n",
    "\n",
    "print(\"Highest Euclidean Value = %s at K=%s\" % (ymax, xmax,))\n",
    "plt.plot(K, euclidean, 'bo-') \n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Euclidean') \n",
    "plt.title('The Silhouette Method using Euclidean Distance') \n",
    "plt.vlines(xmax, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
    "plt.savefig('Grafik/Silhouette Method using Euclidean Distance.png', format='png', dpi=1000)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ymax = max(cosine)\n",
    "xpos = cosine.index(ymax)\n",
    "xmax = K[xpos]\n",
    "\n",
    "print(\"Highest Cosine Value : %s at K=%s\" % (ymax, xmax,))\n",
    "plt.plot(K, cosine, 'bx-') \n",
    "plt.xlabel('Values of K') \n",
    "plt.ylabel('Cosine') \n",
    "plt.title('The Silhouette Method using Cosine') \n",
    "plt.vlines(xmax, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
    "plt.savefig('Grafik/Silhouette Method using Cosine.png', format='png', dpi=1000)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(normalised_df_lrfm, clusters_number, original_df_lrfm):\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = clusters_number, random_state = 1)\n",
    "    kmeans.fit(normalised_df_lrfm)\n",
    "\n",
    "    # Extract cluster labels\n",
    "    cluster_labels = kmeans.labels_\n",
    "        \n",
    "    # Create a cluster label column in original dataset\n",
    "    df_new = original_df_lrfm.assign(Cluster = cluster_labels)\n",
    "    \n",
    "    # Initialise TSNE\n",
    "    model = TSNE(random_state=1)\n",
    "    transformed = model.fit_transform(df_new)\n",
    "    \n",
    "    # Plot t-SNE\n",
    "    plt.title('Flattened Graph of {} Clusters'.format(clusters_number))\n",
    "    sns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=cluster_labels, style=cluster_labels, palette=\"Set1\")\n",
    "    \n",
    "    return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter Plot\n",
    "plt.figure(figsize=(12, 15))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "df_lrfm_k4 = kmeans(norm_lrfm, 4, lrfm)\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "df_lrfm_k5 = kmeans(norm_lrfm, 5, lrfm)\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "df_lrfm_k6 = kmeans(norm_lrfm, 6, lrfm)\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "df_lrfm_k7 = kmeans(norm_lrfm, 7, lrfm)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('Grafik/Cluster-flattened.png', format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snake Plot\n",
    "plt.figure(figsize=(12, 15))\n",
    "\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.title('Snake Plot of K-Means = 4')\n",
    "snake_plot(norm_lrfm, df_lrfm_k4, lrfm)\n",
    "\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.title('Snake Plot of K-Means = 5')\n",
    "snake_plot(norm_lrfm, df_lrfm_k5, lrfm)\n",
    "\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.title('Snake Plot of K-Means = 6')\n",
    "snake_plot(norm_lrfm, df_lrfm_k6, lrfm)\n",
    "\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.title('Snake Plot of K-Means = 7')\n",
    "snake_plot(norm_lrfm, df_lrfm_k7, lrfm)\n",
    "\n",
    "plt.savefig('Grafik/Cluster-snakeplot.png', format='png', dpi=300)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D Plot\n",
    "fig = plt.figure(figsize=(15, 20))\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1, projection='3d')\n",
    "plt.title('3D Plot of K-Means = 4')\n",
    "threeD_plot(norm_lrfm, df_lrfm_k4)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2, projection='3d')\n",
    "plt.title('3D Plot of K-Means = 5')\n",
    "threeD_plot(norm_lrfm, df_lrfm_k5)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3, projection='3d')\n",
    "plt.title('3D Plot of K-Means = 6')\n",
    "threeD_plot(norm_lrfm, df_lrfm_k6)\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "plt.title('3D Plot of K-Means = 7')\n",
    "threeD_plot(norm_lrfm, df_lrfm_k7)\n",
    "\n",
    "plt.savefig('Grafik/Cluster-3DPlot.png', format='png', dpi=300)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pilih dataframe dengan jumlah cluster yang fix dipakai\n",
    "df_cluster_fix = df_lrfm_k4.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mengubah nilai Recency menggunakan 1-R karena merupakan kebalikan dari variabel lain\n",
    "#R asli jika semakin kecil akan semakin bagus\n",
    "df_lrfm_all = pd.merge(df_cluster_fix, norm_lrfm, on='telp', suffixes=('_real', '_norm'))\n",
    "df_lrfm_all['Recency_norm'] = 1-df_lrfm_all['Recency_norm']\n",
    "\n",
    "df_lrfm_all.to_csv(\"Hasil/Ternakmart_Transaction_Clustered_LRFM.csv\")\n",
    "df_lrfm_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrfm_values(df):\n",
    "    lrfm_cluster = df.groupby(['Cluster']).agg(['mean', 'min', 'max']).round(3)\n",
    "    return lrfm_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean at every cluster\n",
    "lrfm_values(df_lrfm_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean from all dataframe\n",
    "df_lrfm_all.mean(axis=0).round(3)"
   ]
  },
  {
   "source": [
    "## Menghitung CLV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nilai bobot didapatkan dari file excel\n",
    "AHP = {\n",
    "    'Length' : 0.52,\n",
    "    'Recency' : 0.095,\n",
    "    'Frequency' : 0.36,\n",
    "    'Monetary' : 0.494}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lrfm_all['CLV'] = (df_lrfm_all['Length_norm']*AHP['Length'] + df_lrfm_all['Recency_norm']*AHP['Recency'] + df_lrfm_all['Frequency_norm']*AHP['Frequency'] + df_lrfm_all['Monetary_norm']*AHP['Monetary'])\n",
    "display(df_lrfm_all['CLV'].describe())\n",
    "\n",
    "#Agar angka lebih cantik dan mudah dibaca, kita kalikan 1000\n",
    "df_lrfm_all['CLV'] = df_lrfm_all['CLV']*100\n",
    "plt.title('Distribution of CLV')\n",
    "sns.distplot(df_lrfm_all['CLV'], kde=False)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Rank CLV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clv_values(df):\n",
    "    clv_cluster = df.groupby(['Cluster']).agg({\n",
    "        'Length_real': ['min', 'max', 'mean'],\n",
    "        'Recency_real': ['min', 'max', 'mean'],\n",
    "        'Frequency_real': ['min', 'max', 'mean'],\n",
    "        'Monetary_real': ['min', 'max', 'mean'],\n",
    "        'CLV' : 'mean'\n",
    "    }).round(0)\n",
    "    \n",
    "    return clv_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clv_values(df_lrfm_all).sort_values(by=[('CLV','mean')], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice data berdasarkan cluster\n",
    "df_lrfm_c0 = df_lrfm_all[lambda x: x['Cluster'] == 0]\n",
    "df_lrfm_c1 = df_lrfm_all[lambda x: x['Cluster'] == 1]\n",
    "df_lrfm_c2 = df_lrfm_all[lambda x: x['Cluster'] == 2]\n",
    "df_lrfm_c3 = df_lrfm_all[lambda x: x['Cluster'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df_lrfm_c0['Length_real'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Basket Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basket_all = pd.merge(df_clean, df_cluster_fix, on='telp', how='left')\n",
    "df_basket_all = df_basket_all[['delivery_date', 'no_order', 'telp', 'prod_id', 'produk', 'qty', 'Cluster']].copy()\n",
    "df_basket_all['prod_id'] = df_basket_all['prod_id'].astype(int)\n",
    "\n",
    "#drop NA from frequency outlier\n",
    "df_basket_all.dropna(subset=['Cluster'], inplace=True)\n",
    "\n",
    "print(\"Dataset dimension : \" + str(df_basket_all.shape))\n",
    "print('Jumlah Customer : ' + str(len(pd.unique(df_basket_all['telp']))))\n",
    "print('Jumlah Transaksi : ' + str(len(pd.unique(df_basket_all['no_order']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing all Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the product names\n",
    "df_basket_all['produk'] = df_basket_all['produk'].str.strip()\n",
    "df_basket_all['no_order'] = df_basket_all['no_order'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split/subsetting dataframe\n",
    "df_basket_cluster0 = df_basket_all[lambda x: x['Cluster'] == 0]\n",
    "df_basket_cluster1 = df_basket_all[lambda x: x['Cluster'] == 1]\n",
    "df_basket_cluster2 = df_basket_all[lambda x: x['Cluster'] == 2]\n",
    "df_basket_cluster3 = df_basket_all[lambda x: x['Cluster'] == 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create basket datafarme from transactions data with each row representing one basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode the basket\n",
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "#create MBA for every cluster\n",
    "def createMBA(basket_data) :\n",
    "    totalTransactions = len(basket_data.index)\n",
    "    totalCustomers = len(pd.unique(basket_data['telp']))\n",
    "    minTransaction = totalTransactions*0.01\n",
    "    min_support_calc = minTransaction/totalTransactions\n",
    "\n",
    "    print('number of customer in cluster is', totalCustomers)\n",
    "    print('number of baskets for analysis is', totalTransactions)\n",
    "    print('minimum support value is ', round(min_support_calc*100, 4), '%')\n",
    "\n",
    "    basket = basket_data.groupby(['no_order', 'produk'])['qty'].sum().unstack().reset_index().fillna(0).set_index('no_order')\n",
    "    basket_sets = basket.applymap(encode_units)\n",
    "    basket_sets.dropna(inplace=True)\n",
    "    basket_sets = basket_sets.astype(int)\n",
    "    # display(basket_sets.head(5))\n",
    "\n",
    "    #create frequent items sets with clculated minimum support\n",
    "    frequent_itemsets = fpgrowth(basket_sets, min_support=min_support_calc, use_colnames=True)\n",
    "    # display(frequent_itemsets.describe())\n",
    "\n",
    "    rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=0)\n",
    "    rules.sort_values('support', ascending = False, inplace = True)\n",
    "\n",
    "    createMBA.rules = rules\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4) :\n",
    "    #slice data\n",
    "    print(\"Market Basket Analysis for Cluster\", i)\n",
    "    basket_data = df_basket_all[lambda x: x['Cluster'] == i]\n",
    "\n",
    "    createMBA(basket_data)\n",
    "\n",
    "    display(createMBA.rules.head(10))\n",
    "    createMBA.rules.to_csv(\"Hasil/Ternakmart_Results_FPGrowth_Cluster_%s.csv\" % (i,), 'a')\n",
    "    # rules[(rules['lift'] >= 0.2) & (rules['confidence'] >= 0.1)].sort_values(by=['confidence', 'lift'], ascending=False).to_csv(\"Data/Ternakmart_Results_FPGrowth_Cluster_%s.xlsx\" % (i,), 'a', newline='')\n",
    "    print(\"\\n \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}